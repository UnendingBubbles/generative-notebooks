{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UnenBubb-StyleGAN3CLIP-v2-3.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUdODS8IH6lG"
      },
      "source": [
        "# **üí´üßô‚Äç‚ôÇÔ∏è StyleGAN3 + CLIP** \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Text-to-Image Portrait-Style Image Generation:** Faces, Metart, Animals, and Cosplay\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDcjhFogHc4m"
      },
      "source": [
        "( +) **StyleGAN3 + CLIP mishin_learning colab** version of based on @*nshepperd1* colab | inteface by [@nn_for_science](\n",
        "https://t.me/nn_for_science) | and \n",
        "[@mishin_learning](https://t.me/mishin_learning) mod for *cut_size*, *cutn* and mainly *differentiable augmentations*.\n",
        "\n",
        "\n",
        "( ++ ) **Addons by** [*UnendingBubbles*](https://github.com/unendingbubbles) : batch, history-view, grid-of-images, output sharpening, cutout_methods, color-to-alpha png, icon export, QOL everywhere, `Runtime > Run All` compliant üòÅ\n",
        "\n",
        "*last updated: 211111*\n",
        "\n",
        "Detailed instructions at the bottom."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoFfcvEYzvwW"
      },
      "source": [
        "mishin resources: [mishin original colab link](https://colab.research.google.com/drive/1LWWlW40-ZHnyhyJgU6xKaSsl8bwCT1wx?usp=sharing#scrollTo=FMwDxUXjoJJW)\n",
        " ~ [mishin blog](https://www.telemetr.me/content/mishin_learning) ~ [–¢–µ–ª–µ–≥—Ä–∞–º-–∫–∞–Ω–∞–ª –ú–∏—à–∏–Ω –õ–µ—Ä–Ω–∏–Ω–≥ ](https://t.me/mishin_learning) ~ \n",
        "[Transformer | –ü—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ ml –∫–æ–º—å—é–Ω–∏—Ç–∏  ](https://transformer.community/) ~ [Transformer | YouTube –ö–∞–Ω–∞–ª —Å –ª–µ–∫—Ü–∏—è–º–∏ –ø–æ Machine Learning](https://www.youtube.com/channel/UCJABbIsR2bd666zFMHzUFTQ)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rkozWsVq2B7"
      },
      "source": [
        "# @title üìú Licensed under the MIT License\n",
        "# Copyright (c) 2021 Katherine Crowson\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  # print(gpu_info) #for more gpu info\n",
        "  !nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K38uyFrv5wo"
      },
      "source": [
        "# @title üìò Install Libraries\n",
        "# @markdown This cell could take a while since it downloads several libraries\n",
        " \n",
        "#limit maxHeight of output\n",
        "from IPython.display import Javascript\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 176})'''))\n",
        "\n",
        "\n",
        "!pip install --upgrade torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchtext>=0.10.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!git clone https://github.com/NVlabs/stylegan3\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!pip install -e ./CLIP\n",
        "!pip install einops ninja\n",
        "!pip install lpips\n",
        "!pip install kornia\n",
        "\n",
        "import sys\n",
        "sys.path.append('./CLIP')\n",
        "sys.path.append('./stylegan3')\n",
        "\n",
        "import io\n",
        "import os, time\n",
        "#more debug info\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import pickle\n",
        "import shutil\n",
        "import kornia.augmentation as K\n",
        "import numpy as np\n",
        "import math\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import requests\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import clip\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "from IPython.display import display\n",
        "from einops import rearrange\n",
        "\n",
        "from google.colab import files\n",
        "import feather\n",
        "import pandas as pd\n",
        "from google.colab import data_table\n",
        "%load_ext google.colab.data_table \n",
        " #restore native spreadsheet\n",
        "# %unload_ext google.colab.data_table \n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "print('Using device:', device, file=sys.stderr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwFwxAi7UBKP"
      },
      "source": [
        "model notes:\n",
        "\n",
        "| **ffhq** | **metfaces**  | **afhqv2** | **cosplay_faces** | | t  | r |\n",
        "|:--------------:|:-----------:|:------------:|:----------:|:-----------:|:------------:|:----------:|\n",
        "| photo faces | portrait painings | animal faces | by [@l4rz](https://twitter.com/l4rz) | ‚Äé  | translation | rotation |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkolgykbNhRl",
        "cellView": "form"
      },
      "source": [
        "#@markdown üßπ Delete all files in `finals` folder, before continuing? (leave unchecked)\n",
        "\n",
        "# %%js\n",
        "# somethin = confirm(\"GAN3+CLIP_formated_batch-c2a-grid-ico Asks:\\n\\nAre you sure you want to delete all 'final' images? üî•\");\n",
        " \n",
        "clear_finals_now = False #@param {type:\"boolean\"}\n",
        "\n",
        "if clear_finals_now == True: \n",
        "   %rm /content/finals/*\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuqrWwj1ovRx"
      },
      "source": [
        "#@markdown #**üí´ StyleGAN3+CLIP Set Parameters**\n",
        "import numpy as np\n",
        "base_url = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/\"\n",
        "#%cd /content/\n",
        "\n",
        "##//////////////////////////////////////////////\n",
        "# üí´ User Parameters\n",
        "#@markdown ___\n",
        "#@markdown **üé≠ Choose a model:**\n",
        "model_name = \"stylegan3-r-ffhq-1024x1024.pkl\" #@param [\"stylegan3-r-afhqv2-512x512.pkl\", \"stylegan3-r-ffhq-1024x1024.pkl\", \"stylegan3-r-ffhqu-1024x1024.pkl\",\"stylegan3-r-ffhqu-256x256.pkl\",\"stylegan3-r-metfaces-1024x1024.pkl\",\"stylegan3-r-metfacesu-1024x1024.pkl\",\"stylegan3-t-afhqv2-512x512.pkl\",\"stylegan3-t-ffhq-1024x1024.pkl\",\"stylegan3-t-ffhqu-1024x1024.pkl\",\"stylegan3-t-ffhqu-256x256.pkl\",\"stylegan3-t-metfaces-1024x1024.pkl\",\"stylegan3-t-metfacesu-1024x1024.pkl\", \"stylegan2-cosplay-faces-512x512-px\"]\n",
        "network_url = base_url + model_name\n",
        "#@markdown Enable `model_mixing` if you want to mix between two models.(matching resolutions)\n",
        "\n",
        "model_mixing = True #@param {type:\"boolean\"}\n",
        "model2_name = \"stylegan3-r-metfaces-1024x1024.pkl\" #@param [\"stylegan3-r-ffhq-1024x1024.pkl\", \"stylegan3-r-ffhqu-1024x1024.pkl\",\"stylegan3-r-metfaces-1024x1024.pkl\",\"stylegan3-r-metfacesu-1024x1024.pkl\",\"stylegan3-t-ffhq-1024x1024.pkl\",\"stylegan3-t-ffhqu-1024x1024.pkl\",\"stylegan3-t-metfaces-1024x1024.pkl\",\"stylegan3-t-metfacesu-1024x1024.pkl\"] \n",
        "#@markdown Slide `proportion` to the right, for more `model_name` influence.\n",
        "proportion  = 0.985 #@param {type:\"slider\", min:0, max:1, step:0.005}\n",
        "#@markdown ___\n",
        "#@markdown **üí´ Enter your text prompt here:** \n",
        "\n",
        "# text = 'A sexy blonde marilyn monroe pop art in style of andy warhol professional art' #mishin\n",
        "text = 'Carl Sagan in space, and happy. Carl Sagan in the cosmic ocean.' #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "steps =  45#@param {type:\"integer\"}\n",
        "n_batches = 12#@param {type:\"integer\"}\n",
        "# @markdown (fix = less subject movement)\n",
        "fix_coordinates = \"False\" #@param [\"True\", \"False\"]\n",
        "# @markdown (-1 for random seed)\n",
        "seed =  -1#@param {type:\"integer\"}\n",
        "sample_filetype = \"png\" #@param [\"jpg\", \"png\"]\n",
        "\n",
        "\n",
        "show_every_n_steps =  22#@param\n",
        "stats_for_nerds = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ___\n",
        "#@markdown **Advanced Settings:**\n",
        "\n",
        "#@markdown Changing the `cutout_method` greatly influences how the image evolves over time.\n",
        "cutout_method = \"nsheppard\" #@param [\"mishin\", \"nsheppard\"]\n",
        "\n",
        "\n",
        "#@markdown 32-128 works well for `u_cutn`. mishin default of 80. Cut number affects on 'Creativity': less cutout will lead to more random/creative results, sometimes barely readable, while higher values create stable/complex results. üíÄ If CUDA crash, change to 40 or less. \n",
        "u_cutn = 80 #@param {type:\"integer\"} \n",
        "\n",
        "#@markdown Values of `u_cut_pow` below 1 prioritize structure over detail, and vice versa for above 1. mishin default 0.1, torch default 1.0,  UnenBubb default 2.5 (highly dependent on you promt)\n",
        "u_cut_pow = 1.0 #@param {type:\"number\"}  default 1.0. 0.1-3.0 work okay\n",
        "\n",
        "#@markdown Step size is learning rate: mishin default of 0.03. torch default of 0.001\n",
        "u_step_size = 0.03 #@param {type:\"number\"} default 0.03\n",
        "\n",
        "#@markdown Something to do with setup random connections/loss. Default 8. üíÄ If CUDA crash, use 4. \n",
        "u_t_ran = \"8\" #@param [4, 8]\n",
        "\n",
        "#@markdown Betas: coefficients used for computing running averages of gradient and its square. torch default: 0.9, 0.999. mishin default: 0.0, 0.9. mishin alt: 0.0, 0.999 or 0.8, 0.9. \n",
        "\n",
        "u_beta_a = 0.0 #@param {type:\"number\"}\n",
        "\n",
        "u_beta_b = 0.9 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown ___\n",
        "\n",
        "save_into_history = True #@param {type:\"boolean\"}\n",
        "clear_history = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e3A8pPr0FtE"
      },
      "source": [
        "#@markdown #**üèÉ‚Äç‚ôÄÔ∏è Run StyleGAN3+CLIP**\n",
        "#@markdown Images placed in `samples` and `finals` folder to your left.\n",
        "\n",
        "\n",
        "# You can doubleclick the grey area to the right to hide code.\n",
        "##//////////////////////////////////////////////\n",
        "# Parameter Setup\n",
        "\n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "def fetch_model(url_or_path):\n",
        "    basename = os.path.basename(url_or_path)\n",
        "    if os.path.exists(basename):\n",
        "        return basename\n",
        "    else:\n",
        "        !wget -c '{url_or_path}'\n",
        "        return basename\n",
        "\n",
        "if model_name == \"stylegan2-cosplay-faces-512x512-px\":\n",
        "    network_url = 'https://l4rz.net/cosplayface-snapshot-004000-18160-FID367.pkl'\n",
        "    \n",
        "with open(fetch_model(network_url), 'rb') as fp:\n",
        "  G = pickle.load(fp)['G_ema'].to(device)\n",
        "\n",
        "u_t_ran = int(u_t_ran)\n",
        "\n",
        "seed_orig = seed\n",
        "text_safe = text\n",
        "text_safe = text_safe.replace(\" \", \"_\")\n",
        "# text_safe = text_safe.replace(\" \", \"%\")\n",
        "\n",
        "\n",
        "if fix_coordinates == \"True\":\n",
        "    if model_name != \"stylegan2-cosplay-faces-512x512-px\":\n",
        "        shift = G.synthesis.input.affine(G.mapping.w_avg.unsqueeze(0))\n",
        "        G.synthesis.input.affine.bias.data.add_(shift.squeeze(0))\n",
        "        G.synthesis.input.affine.weight.data.zero_()\n",
        "\n",
        "# target = embed_url(\"https://4.bp.blogspot.com/-uw859dFGsLc/Va5gt-bU9bI/AAAAAAAA4gM/dcaWzX0ZxdI/s1600/Lubjana+dragon+1.jpg\")\n",
        "# target = embed_url(\"https://irc.zlkj.in/uploads/e399d2fee2c6edd9/20210827165231_0_nexus%20of%20abandoned%20places.%20trending%20on%20ArtStation.png\")\n",
        "\n",
        "\n",
        "\n",
        "# escaping random noise is comfortable for everyone\n",
        "#limit maxHeight of output\n",
        "from IPython.display import Javascript\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 495})'''))\n",
        "\n",
        "##//////////////////////////////////////////////\n",
        "# CLIP for GAN3 Setup\n",
        "\n",
        "def norm1(prompt):\n",
        "    \"Normalize to the unit sphere.\"\n",
        "    return prompt / prompt.square().sum(dim=-1,keepdim=True).sqrt()\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "if cutout_method == 'nsheppard':\n",
        "  class MakeCutouts(torch.nn.Module):\n",
        "      def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "          print(f'cutn:{cutn}, cut_pow:{cut_pow}')\n",
        "          super().__init__()\n",
        "          self.cut_size = cut_size\n",
        "          self.cutn = cutn\n",
        "          self.cut_pow = cut_pow\n",
        "\n",
        "      def forward(self, input):\n",
        "          sideY, sideX = input.shape[2:4]\n",
        "          max_size = min(sideX, sideY)\n",
        "          min_size = min(sideX, sideY, self.cut_size)\n",
        "          cutouts = []\n",
        "          for _ in range(self.cutn):\n",
        "              size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "              offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "              offsety = torch.randint(0, sideY - size + 1, ())\n",
        "              cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "              cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
        "          return torch.cat(cutouts)\n",
        "\n",
        "if cutout_method == 'mishin':\n",
        "  class MakeCutouts(torch.nn.Module):\n",
        "      def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "          print(f'cutn:{cutn}, cut_pow:{cut_pow}')\n",
        "          super().__init__()\n",
        "          self.cut_size = cut_size\n",
        "          self.cutn = cutn\n",
        "          self.cut_pow = cut_pow\n",
        "          self.augs = torch.nn.Sequential(\n",
        "              K.RandomHorizontalFlip(p=0.5),\n",
        "              K.RandomSharpness(0.3,p=0.4),\n",
        "              K.RandomAffine(degrees=7, translate=0.05, p=0.7, padding_mode='border'),\n",
        "              K.RandomPerspective(0.2, p=0.5),\n",
        "              K.ColorJitter(hue=0.01, saturation=0.01, p=0.7))\n",
        "\n",
        "      def forward(self, input):\n",
        "          sideY, sideX = input.shape[2:4]\n",
        "          max_size = min(sideX, sideY)\n",
        "          min_size = min(sideX, sideY, self.cut_size)\n",
        "          cutouts = []\n",
        "          for _ in range(self.cutn):\n",
        "              size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "              offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "              offsety = torch.randint(0, sideY - size + 1, ())\n",
        "              cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "              # cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "              cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
        "          batch = self.augs(torch.cat(cutouts, dim=0))\n",
        "          return batch\n",
        "\n",
        "\n",
        "make_cutouts = MakeCutouts(224, u_cutn, u_cut_pow)\n",
        "\n",
        "def embed_image(image):\n",
        "  n = image.shape[0]\n",
        "  cutouts = make_cutouts(image)\n",
        "  embeds = clip_model.embed_cutout(cutouts)\n",
        "  embeds = rearrange(embeds, '(cc n) c -> cc n c', n=n)\n",
        "  return embeds\n",
        "\n",
        "def embed_url(url):\n",
        "  image = Image.open(fetch(url)).convert('RGB')\n",
        "  return embed_image(TF.to_tensor(image).to(device).unsqueeze(0)).mean(0).squeeze(0)\n",
        "\n",
        "class CLIP(object):\n",
        "  def __init__(self):\n",
        "    clip_model = \"ViT-B/32\"\n",
        "    self.model, _ = clip.load(clip_model)\n",
        "    self.model = self.model.requires_grad_(False)\n",
        "    self.normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                          std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def embed_text(self, prompt):\n",
        "      \"Normalized clip text embedding.\"\n",
        "      return norm1(self.model.encode_text(clip.tokenize(prompt).to(device)).float())\n",
        "\n",
        "  def embed_cutout(self, image):\n",
        "      \"Normalized clip image embedding.\"\n",
        "      return norm1(self.model.encode_image(self.normalize(image)))\n",
        "  \n",
        "clip_model = CLIP()\n",
        "target = clip_model.embed_text(text) \n",
        "\n",
        "if model_mixing == True:\n",
        "  with open(fetch_model(base_url + model_name), 'rb') as fp:\n",
        "      G1 = pickle.load(fp)['G_ema'].to(device)\n",
        "\n",
        "  with open(fetch_model(base_url + model2_name), 'rb') as fp:\n",
        "      G2 = pickle.load(fp)['G_ema'].to(device)\n",
        "\n",
        "  G = G1\n",
        "  for p_out, p_in1, p_in2 in zip(G.parameters(), G1.parameters(), G2.parameters()):\n",
        "      p_out.data = torch.nn.Parameter(p_in1*proportion+p_in2*(1-proportion));\n",
        "  print(f'model mixing: {model_name} x {model2_name} [{proportion}:{1-proportion}]')\n",
        "else:\n",
        "  print(f'model: {model_name}')\n",
        "\n",
        "zs = torch.randn([10000, G.mapping.z_dim], device=device)\n",
        "w_stds = G.mapping(zs, None).std(0)\n",
        "\n",
        "\n",
        "##//////////////////////////////////////////////\n",
        "# Generate image\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "def batching(timestring):\n",
        "\n",
        "  if seed_orig == -1:\n",
        "    seed = np.random.randint(0, 2**32 - 1)\n",
        "    \n",
        "  tf = Compose([\n",
        "    Resize(224),\n",
        "    lambda x: torch.clamp((x+1)/2,min=0,max=1),\n",
        "    ])\n",
        "\n",
        "  torch.manual_seed(seed)\n",
        "  # timestring = 'generation' #   timestring = time.strftime('%Y%m%d%H%M%S')\n",
        "  rand_z = torch.from_numpy(np.random.RandomState(seed).randn(1, G.mapping.z_dim)).to(device)\n",
        "  q = (G.mapping(rand_z, None, truncation_psi=0.2)) / w_stds\n",
        "  q.requires_grad_()\n",
        "    \n",
        "  with torch.no_grad():\n",
        "      qs = []\n",
        "      losses = []\n",
        "      nn_loop = tqdm(range(0, 4))\n",
        "      for _ in nn_loop:\n",
        "          # q = (G.mapping(torch.randn([8, G.mapping.z_dim], device=device), None, truncation_psi=0.7) - G.mapping.w_avg) / w_stds #if memory issue, change 8 to 4\n",
        "          q = (G.mapping(torch.randn([u_t_ran, G.mapping.z_dim], device=device), None, truncation_psi=0.7) - G.mapping.w_avg) / w_stds #if memory issue, change 8 to 4\n",
        "          images = G.synthesis(q * w_stds + G.mapping.w_avg)\n",
        "          embeds = embed_image(images.add(1).div(2))\n",
        "          loss = spherical_dist_loss(embeds, target).mean(0)\n",
        "          i = torch.argmin(loss)\n",
        "          qs.append(q[i])\n",
        "          losses.append(loss[i])\n",
        "      qs = torch.stack(qs)\n",
        "      losses = torch.stack(losses)\n",
        "      i = torch.argmin(losses)\n",
        "      q = qs[i].unsqueeze(0).requires_grad_()\n",
        "\n",
        "  # Sampling loop\n",
        "  q_ema = q\n",
        "  #https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\n",
        "  opt = torch.optim.AdamW([q], u_step_size, betas=(u_beta_a, u_beta_b))\n",
        "  # opt = torch.optim.AdamW([q], lr=0.03, betas=(0.0, 0.9)) # 0.0, 0.999 or 0.8, 0.9, lr 0.02~0.04 def lr=0.03\n",
        "\n",
        "\n",
        "  loop = tqdm(range(0, steps))\n",
        "  for i in loop:\n",
        "      nifty_name = f'{timestring}-s{str(seed)}-i{str(steps)}'\n",
        "      opt.zero_grad()\n",
        "      w = q * w_stds\n",
        "      image = G.synthesis(w + G.mapping.w_avg, noise_mode='const')\n",
        "      embed = embed_image(image.add(1).div(2))\n",
        "      loss = spherical_dist_loss(embed, target).mean()\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "      loop.set_postfix(loss=loss.item(), q_magnitude=q.std().item())\n",
        "\n",
        "      q_ema = q_ema * 0.9 + q * 0.1  # was 0.9, 0.1\n",
        "      image = G.synthesis(q_ema * w_stds + G.mapping.w_avg, noise_mode='const')\n",
        "\n",
        "      if i % show_every_n_steps == 0:\n",
        "          if stats_for_nerds == True:\n",
        "            # kindloss = loss[6:]\n",
        "            print(f\"\\n Image {i}/{steps} | Batch {k+1}/{n_batches} | Current loss: {loss} | {nifty_name}\") \n",
        "          else:\n",
        "            print('\\n')\n",
        "          display(TF.to_pil_image(tf(image)[0]))\n",
        "      pil_image = TF.to_pil_image(image[0].add(1).div(2).clamp(0,1))\n",
        "      os.makedirs(f'samples/{timestring}', exist_ok=True)\n",
        "      pil_image.save(f'samples/{timestring}/{i:04}.{sample_filetype}')\n",
        "\n",
        "      # insert save last numbered iteration\n",
        "      if i == steps - 1:\n",
        "        os.makedirs(f'finals', exist_ok=True)\n",
        "        pil_image.save(f'finals/{nifty_name}.{sample_filetype}')\n",
        "        # info_pass = timestring + \"-i\" + str(steps)\n",
        "  if stats_for_nerds == True:         \n",
        "    print(f\"\\n Image {i+1}/{steps} | Batch {k+1}/{n_batches} | Current loss: {loss} | {nifty_name}\")  #added batch and info_pass \n",
        "  else:\n",
        "    print(f'{nifty_name}')\n",
        "  display(TF.to_pil_image(tf(image)[0]))\n",
        "     \n",
        "##//////////////////////////////////////////////\n",
        "# Batching Loop\n",
        "\n",
        "timestring = time.strftime('%Y%m%d%H%M%S')\n",
        "\n",
        "print(text)\n",
        "\n",
        "try:\n",
        "  for k in range(n_batches): \n",
        "    info_pass = f'{timestring}-i{str(steps)}'\n",
        "    timestring = time.strftime('%Y%m%d%H%M%S')\n",
        "    batching(timestring)\n",
        "except KeyboardInterrupt:  #ability to stop batch without unresponsive error\n",
        "  pass\n",
        "\n",
        "# Save images as a tar archive\n",
        "!tar cf samples/{timestring}.tar samples/{timestring}\n",
        "#@markdown ___\n",
        "\n",
        "##//////////////////////////////////////////////\n",
        "#History View\n",
        "\n",
        "data_l = ['timestring','model_name','mixing','model2_name','proportion','text','steps','n_batches','fix','seed','sample_filetype','method','cutn','cut_pow','step_size','u_t_ran','u_beta_a','u_beta_b'] #column list\n",
        "data_ev_l = [timestring,model_name,model_mixing,model2_name,proportion,text,steps,n_batches,fix_coordinates,seed,sample_filetype,cutout_method,u_cutn,u_cut_pow,u_step_size,u_t_ran,u_beta_a,u_beta_b]\n",
        "\n",
        "from google.colab import data_table\n",
        "# %load_ext google.colab.data_table \n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    big_history\n",
        "except NameError:\n",
        "    big_history = [] #initialize\n",
        "\n",
        "if clear_history == True:\n",
        "  big_history = []\n",
        "\n",
        "if save_into_history == True:\n",
        "  big_history.append(data_ev_l)\n",
        "  # print(big_history)\n",
        "  history_df = pd.DataFrame(big_history, columns=data_l) \n",
        "\n",
        "#tip: click here and press Ctrl+M+P (in that order) to scroll to top of cell\n",
        "#     (or doubleclick the grey area to the right to hide code)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy2L1MGLteP6"
      },
      "source": [
        "#@markdown #‚è≥ History View: üí´ prompt\n",
        "data_table.DataTable(history_df[[\"text\", \"timestring\"]].tail(3), num_rows_per_page=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDMAJAwCT9_m"
      },
      "source": [
        "#@markdown #‚è≥ History View: üí´ useful settings\n",
        "data_table.DataTable(history_df[['model_name','mixing','model2_name','proportion','steps','n_batches','fix','seed','method','cutn','cut_pow','step_size']].tail(3), num_rows_per_page=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTYCoN9uSnBJ"
      },
      "source": [
        "#@markdown #**üî≥ Create a Grid of Images**\n",
        "#@markdown Files placed in `gridded` folder to your left.\n",
        "\n",
        "from IPython.display import Image\n",
        "import PIL, os, glob\n",
        "from PIL import Image, ImageEnhance\n",
        "from PIL import ImageDraw\n",
        "from math import ceil, floor, trunc\n",
        "from google.colab import files\n",
        "\n",
        "generate_grid = True #@param {type:\"boolean\"}\n",
        "destination = \"/content/grided\"\n",
        "destination_presharp = \"/content/finals-sharp\"\n",
        "\n",
        "filetype_in = \"png\" #@param [\"jpg\", \"png\"]\n",
        "filetype_out = \"jpg\" #@param [\"jpg\", \"png\"]\n",
        "#@markdown Enable `pixel_perfect` to disable resizing of images. If you want to set a custom final output size (frame_width), this must be disabled. Even with `pixel_perfect` enabled, images that are different aspect ratio or larger size from the first, will be resized to fit.\n",
        "pixel_perfect = True #@param {type:\"boolean\"}\n",
        "frame_width = 2000 #@param {type:\"number\"}\n",
        "max_imgs =  32#@param {type:\"number\"}\n",
        "images_per_row =  4#@param {type:\"number\"}\n",
        "padding =  18#@param {type:\"number\"}\n",
        "boarder = 40 #@param {type:\"number\"}\n",
        "bgRed = 30 #@param {type:\"number\"} \n",
        "bgGreen = 30 #@param {type:\"number\"} \n",
        "bgBlue =  30#@param {type:\"number\"} \n",
        "\n",
        "\n",
        "#@markdown ‚úã Warning: Session may crash if trying to display+download files above 5MB. Comment out the last lines to prevent that, when exporting large PNGs. You can manually download them from the `gridded` folder on the left.\n",
        "download_when_complete = True  #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ___\n",
        "#@markdown **Sharpening:** might only be useful if images are very downscaled, generated from non-sharpend datasets, or viewed on an imperfect medium. Sharpening is always a destructive process, but so is looking at a computer monitor.\n",
        "\n",
        "#@markdown (Values above 1.00 sharpen, below 1.00 to blur.)\n",
        "\n",
        "#@markdown **üåü Pre-sharpening** to be applied to original input images, durring processing, before images are put into a grid. `pre_sharpening` is more useful durring downscaling.\n",
        "\n",
        "apply_pre_sharp = False #@param {type:\"boolean\"}\n",
        "#@markdown  (Values of 1.10-1.50 when downscaling. Less if `pixel_perfect` enabled)\n",
        "pre_sharpening = 1.15 #@param {type:\"number\"}  \n",
        "#@markdown  Save all those pre-sharpened originals in `finals-sharp` folder?\n",
        "save_pre_sharps = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown **üåü Post-sharpening** to be applied 1:1 on final `gridded` result.\n",
        "apply_post_sharp = False #@param {type:\"boolean\"}\n",
        "#@markdown (Values of 1.05-1.40 work for viewing on screen. More for paper printing.)\n",
        "post_sharpening = 1.10 #@param {type:\"number\"}  \n",
        "\n",
        "#@markdown If you can notice the sharpening without zooming to 1:1 pixels, it's probably too much sharpening. Windows tip: press Ctrl+1 in windows-photo-viewer to view at 100% scale, after opening.\n",
        "\n",
        "\n",
        "images = glob.glob(\"/content/finals/*.\" + filetype_in)\n",
        "images = images[:max_imgs]\n",
        "\n",
        "os.makedirs(f'{destination}', exist_ok=True)\n",
        "full_destination = f'{destination}/{info_pass}-{text_safe}-{str(len(images))}up.{filetype_out}'\n",
        "\n",
        "def gridcreator(destination, frame_width):\n",
        "\n",
        "  images = glob.glob(\"/content/finals/*.\" + filetype_in)\n",
        "  images = images[:max_imgs]                #get the first n# images\n",
        "  images.sort(key=os.path.getctime)         #sort files by date\n",
        "\n",
        "\n",
        "  img_width, img_height = Image.open(images[0]).size\n",
        "  sf = (frame_width-(images_per_row-1)*padding)/(images_per_row*img_width)       #scaling factor\n",
        "  scaled_img_width = ceil(img_width*sf)                  \n",
        "  scaled_img_height = ceil(img_height*sf) + padding\n",
        "  number_of_rows = ceil(len(images)/images_per_row)\n",
        "\n",
        "  if pixel_perfect == True:\n",
        "    scaled_img_width = img_width\n",
        "    scaled_img_height = img_height + padding\n",
        "    frame_width = images_per_row * (img_width) + ((images_per_row-1) * padding)\n",
        "\n",
        "  #frame_height = ceil((sf*img_height+padding)*number_of_rows) \n",
        "  frame_height = ceil(scaled_img_height*number_of_rows)\n",
        "\n",
        "  new_im = Image.new('RGB', (frame_width+boarder*2, frame_height+boarder*2-padding), (bgRed, bgGreen, bgBlue)) \n",
        "\n",
        "  i,j=0,0\n",
        "  for num, im in enumerate(images):\n",
        "      if num%images_per_row==0:\n",
        "          i=0\n",
        "      im = Image.open(im)\n",
        "\n",
        "      if apply_pre_sharp == 1:\n",
        "        enhancer = ImageEnhance.Sharpness(im)\n",
        "        # enhanced_im = enhancer.enhance(pre_sharpening)\n",
        "        im = enhancer.enhance(pre_sharpening)\n",
        "        if save_pre_sharps == 1:\n",
        "          os.makedirs(f'{destination_presharp}', exist_ok=True)\n",
        "          if filetype_out == \"jpg\":\n",
        "            new_im.save(f'{destination_presharp}/{info_pass}-{text_safe}-{i}', \"JPEG\", quality=80, optimize=True, progressive=True)\n",
        "          if filetype_out == \"png\":\n",
        "            new_im.save(f'{destination_presharp}/{info_pass}-{text_safe}-{i}', \"PNG\", quality=80, optimize=True, progressive=True)\n",
        "        #im = enhanced_im\n",
        "      im.thumbnail((scaled_img_width,scaled_img_height), resample=3, reducing_gap=3)\n",
        "  #    im.thumbnail((scaled_img_width,scaled_img_height))\n",
        "      y_cord = (j//images_per_row)*scaled_img_height\n",
        "      new_im.paste(im, (i+boarder,y_cord+boarder))\n",
        "      #print(i, y_cord)\n",
        "      i=(i+scaled_img_width)+padding\n",
        "      j+=1\n",
        "\n",
        "  # TODO enable text\n",
        "  # Call draw Method to add 2D graphics in an image\n",
        "  #I1 = ImageDraw.Draw(new_im)\n",
        "  # Add Text to an image\n",
        "  #I1.text((28, 36), \"nice Car\", fill=(255, 0, 0))\n",
        "\n",
        "  if apply_post_sharp == False:\n",
        "    if filetype_out == \"jpg\":\n",
        "      new_im.save(full_destination, \"JPEG\", quality=85, optimize=True, progressive=True)\n",
        "    if filetype_out == \"png\":\n",
        "      new_im.save(full_destination, \"PNG\", quality=85, optimize=True, progressive=True)\n",
        "    if download_when_complete == True:\n",
        "      print('download requet sent...')\n",
        "      files.download(full_destination)\n",
        "  else:\n",
        "    enhancer = ImageEnhance.Sharpness(new_im)\n",
        "    enhanced_im = enhancer.enhance(post_sharpening)\n",
        "\n",
        "    if filetype_out == \"jpg\":\n",
        "      enhanced_im.save(full_destination, \"JPEG\", quality=85, optimize=True, progressive=True)\n",
        "    if filetype_out == \"png\":\n",
        "      enhanced_im.save(full_destination, \"PNG\", quality=85, optimize=True, progressive=True)\n",
        "    if download_when_complete == True:\n",
        "      print('download requet sent...')\n",
        "      files.download(full_destination)\n",
        "\n",
        "  print(f\"{full_destination} - {os.path.getsize(full_destination)/1048576:.3f} MB\")\n",
        "\n",
        "try:\n",
        "  if generate_grid == True:\n",
        "    #print('generating grid...')\n",
        "    gridcreator(destination, frame_width)  #it gets upset if you dont pass these variables for some reason\n",
        "    #Image(full_destination)\n",
        "  else:\n",
        "    print('check \"generate_grid\" to run this feature.')\n",
        "except KeyboardInterrupt:  #ability to stop without unresponsive error\n",
        "  pass\n",
        "\n",
        "print(text)\n",
        "full_destination = f'{destination}/{info_pass}-{text_safe}-{str(len(images))}up.{filetype_out}'\n",
        "print(\"\\n\")\n",
        "# from IPython.display import Image      #Colab will softcrash when displaying very large images\n",
        "# Image(full_destination)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubJ_SSv2gyui"
      },
      "source": [
        "#@markdown #üíæüí´ export current settings history into csv\n",
        "#################################\n",
        "\n",
        "export_to_csv = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ‚úã If your runtime is restarted, the history will be cleared, and the csv will be overwritten as empty.\n",
        "\n",
        "#@markdown If no `...history_df.csv` is found, one will be created.\n",
        "\n",
        "if export_to_csv == True:\n",
        "  import glob\n",
        "  if not glob.glob('*ory_df.csv'): #if no csv exists, create one\n",
        "    # history_df = \"\"  #do not set this to None...\n",
        "    timestring4 = time.strftime('%Y%m%d-%H%M')\n",
        "    history_csv_at = f'{timestring4}_history_df.csv'\n",
        "    print(f'creating new history log: {history_csv_at}')\n",
        "    history_df.to_csv(f'/content/{history_csv_at}')\n",
        "\n",
        "  print(f'overwriting to history log: {history_csv_at}')\n",
        "  history_df.to_csv(f'/content/{history_csv_at}')\n",
        "\n",
        "#to import an old history, run this line\n",
        "  # history_df = pd.read_csv('path_to_csv_file')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2sqcAAmO42w"
      },
      "source": [
        "#@title **üé• Generate a `.mp4` video**\n",
        "#@markdown Make a video of one batch. Files placed in `videos` folder to your left.\n",
        "\n",
        "from PIL import ImageTk, Image\n",
        "from tkinter import Tk\n",
        "#https://stackoverflow.com/questions/61827899/attributeerror-image-has-no-attribute-open\n",
        "\n",
        "from IPython import display\n",
        "from base64 import b64encode\n",
        "from google.colab import files\n",
        "\n",
        "#User Parameters\n",
        "generate_video = False #@param {type:\"boolean\"}\n",
        "#@markdown Leave empty for last run. Path to images (`000x.jpg` / `000x.png`):\n",
        "folder_of_images = '' #@param {type:\"string\"}\n",
        "filetype_in = \"jpg\" #@param [\"jpg\", \"png\"]\n",
        "#@markdown Desired time of the video in seconds (min fps is 10, so your video might be shorter then desired):\n",
        "length =  5#@param \n",
        "#@markdown Leave unchecked for full length video. Frames to start and stop.\n",
        "custom_frames = False #@param {type:\"boolean\"}\n",
        "init_frame = 1 #@param {type:\"integer\"}\n",
        "last_frame = 4 #@param {type:\"integer\"}\n",
        "download_video_when_complete = False  #@param {type:\"boolean\"}\n",
        "\n",
        "#Generate Video\n",
        "data_url = None #must declare, or else HTML parse throws error when generate_video disable\n",
        "\n",
        "if folder_of_images != '':\n",
        "  directory_in = folder_of_images\n",
        "  timestring = folder_of_images[-14:]\n",
        "else:\n",
        "  directory_in = f'samples/{timestring}'\n",
        "\n",
        "if custom_frames == True:\n",
        "  total_frames = last_frame - init_frame\n",
        "else:\n",
        "  total_frames = steps\n",
        "\n",
        "min_fps = 10\n",
        "max_fps = 30\n",
        "\n",
        "if generate_video == True:\n",
        "  frames = []\n",
        "  print('Generating video...')\n",
        "  if custom_frames == True:\n",
        "    for i in range(init_frame,last_frame): \n",
        "      filename = f\"{directory_in}/{i:04}.{filetype_in}\"\n",
        "      frames.append(Image.open(filename))\n",
        "  else:\n",
        "    for i in sorted(os.listdir(directory_in)): \n",
        "        frames.append(Image.open(f'{directory_in}/{i}'))\n",
        "\n",
        "  fps = np.clip(total_frames/length,min_fps,max_fps)\n",
        "\n",
        "  os.makedirs(f'videos', exist_ok=True)\n",
        "  savepath=f'videos/{timestring}.mp4'\n",
        "  from subprocess import Popen, PIPE\n",
        "  p = Popen(['ffmpeg', '-y', '-f', 'image2pipe', '-vcodec', 'png', '-r', str(fps), '-i', '-', '-vcodec', 'libx264', '-r', str(fps), '-pix_fmt', 'yuv420p', '-crf', '17', '-preset', 'veryslow', savepath], stdin=PIPE)\n",
        "  for im in tqdm(frames):\n",
        "      im.save(p.stdin, 'PNG')\n",
        "  p.stdin.close()\n",
        "  p.wait()\n",
        "\n",
        "  print(f\"{savepath} - {os.path.getsize(full_destination)/1048576:.3f} MB\")\n",
        "  print(text)\n",
        "  print(\"\\n\")\n",
        "\n",
        "  if download_video_when_complete == True:\n",
        "    files.download(full_destination)\n",
        "\n",
        "  mp4 = open(savepath,'rb').read()\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "display.HTML(\"\"\"\n",
        "<video width=640 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KpTZxIOnHJX"
      },
      "source": [
        "#@markdown üßπ Delete all files in `transparent` folder, before continuing? (leave unchecked)\n",
        "\n",
        "# %%js\n",
        "# somethin = confirm(\"GAN3+CLIP_formated_batch-c2a-grid-ico Asks:\\n\\nAre you sure you want to delete all 'final' images? üî•\");\n",
        " \n",
        "clear_transparent_now = False #@param {type:\"boolean\"}\n",
        "\n",
        "if clear_transparent_now == True: \n",
        "   %rm /content/transparent/*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtRIK6fxksGr"
      },
      "source": [
        "#@markdown #**üî¨Color Range to Alpha**\n",
        "import os\n",
        "import fnmatch\n",
        "\n",
        "#@markdown Everything close to chosen color becomes transparent. Processes everything in `path_to_originals` folder into new `.png` in `tranparent` folder to your left.  \n",
        "#Todo: add more options. fix minor ghosting.\n",
        "\n",
        "generate_transparents = False #@param {type:\"boolean\"}\n",
        "path_to_originals = '/content/finals/' #@param {type:\"string\"}\n",
        "c_range = 120 #@param {type:\"integer\"}\n",
        "\n",
        "cRed = 0 #@param {type:\"number\"} \n",
        "cGreen = 0 #@param {type:\"number\"} \n",
        "cBlue = 0#@param {type:\"number\"} \n",
        "\n",
        "if generate_transparents == True: \n",
        "\n",
        "  for path,dirs,files in os.walk('/content/finals/'):\n",
        "      for file in files:\n",
        "          if fnmatch.fnmatch(file,'*.*'):\n",
        "              fullname = os.path.join(path,file)\n",
        "              #print(fullname)\n",
        "              lessname = file[:-4]\n",
        "\n",
        "              #https://www.geeksforgeeks.org/how-to-make-background-image-transparent-using-python/\n",
        "              #happy if-style color to alpha3\n",
        "              from PIL import Image\n",
        "                \n",
        "              img = Image.open(fullname)\n",
        "              rgba = img.convert(\"RGBA\")\n",
        "              datas = rgba.getdata()\n",
        "                \n",
        "              newData = []\n",
        "              for item in datas:\n",
        "                  #if item[0] == 0 and item[1] == 0 and item[2] == 0:  # finding black colour by its RGB value #default\n",
        "                      # storing a transparent value when we find a black colour #default\n",
        "                      #newData.append((255, 255, 255, 0)) #default\n",
        "                  # if item[0] <= c_range and item[1] <= c_range and item[2] <= c_range:  # finding black colour by its RGB value\n",
        "                  #if abs(item[0]-cRed) <= c_range and abs(item[1]-cBlue) <= c_range and abs(item[2])-cBlue <= c_range:  # finding black colour by its RGB value\n",
        "                  if abs(item[0]-cRed) <= c_range or abs(item[1]-cBlue) <= c_range or abs(item[2])-cBlue <= c_range:  # finding black colour by its RGB value\n",
        "\n",
        "                      # storing a transparent value when we find a black colour\n",
        "                      # avgy = int(((item[0] + item[1] + item[2])/3) * (255/c_range))\n",
        "                      avgy = int(((abs(item[0]-cRed) + abs(item[1]-cBlue) + abs(item[2])-cBlue)/3) * (255/c_range))\n",
        "                      newData.append((item[0], item[1], item[2], avgy))\n",
        "                  else:\n",
        "                      newData.append(item)  # other colours remain unchanged\n",
        "                \n",
        "              rgba.putdata(newData)\n",
        "              os.makedirs(f'transparent', exist_ok=True)\n",
        "              savepath = \"/content/transparent/\" + lessname + \"transp.png\"\n",
        "              rgba.save(savepath, \"PNG\")\n",
        "              #print(savepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58FAMLMkyqOS"
      },
      "source": [
        "#@markdown üßπ Delete all files in `icons` folder, before continuing? (leave unchecked)\n",
        "\n",
        "# %%js\n",
        "# somethin = confirm(\"GAN3+CLIP_formated_batch-c2a-grid-ico Asks:\\n\\nAre you sure you want to delete all 'final' images? üî•\");\n",
        " \n",
        "clear_icons_now = False #@param {type:\"boolean\"}\n",
        "\n",
        "if clear_icons_now == True: \n",
        "   %rm /content/icons/*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-iOpAXDks_8",
        "cellView": "form"
      },
      "source": [
        "#@markdown #**üìê Create .ico files**\n",
        "#@markdown Generate icons for your desktop üòÅ\n",
        "#@markdown These `.ico` files contain everything from 16x16 to 256x256. Files placed in `icons` and `icons_zipped` folders to your left.\n",
        "\n",
        "import os\n",
        "import fnmatch\n",
        "\n",
        "generate_icons = False #@param {type:\"boolean\"}\n",
        "zip_icons = True #@param {type:\"boolean\"}\n",
        "\n",
        "if generate_icons == True:\n",
        "\n",
        "  timestring = time.strftime('%Y%m%d%H%M%S')\n",
        "  os.makedirs(f'icons', exist_ok=True)\n",
        "  for path,dirs,files in os.walk('/content/transparent/'):\n",
        "      for file in files:\n",
        "          if fnmatch.fnmatch(file,'*.png'):\n",
        "              fullname = os.path.join(path,file)\n",
        "              #print(fullname)\n",
        "              lessname = file[:-4]\n",
        "\n",
        "              from PIL import Image\n",
        "              img = Image.open(fullname)\n",
        "              savepath = \"/content/icons/\" + lessname + \"icon.ico\"\n",
        "              img.save(savepath)\n",
        "              #print(savepath)\n",
        "  if zip_icons == True:\n",
        "    os.makedirs(f'icons_zipped', exist_ok=True)\n",
        "    import shutil\n",
        "    shutil.make_archive(f'/content/icons_zipped/icons_{timestring}', 'zip', '/content/icons/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cqA6S4OuMhQ",
        "cellView": "form"
      },
      "source": [
        "#@markdown other history-view options\n",
        "# data_table.DataTable(history_df[[\"text\", \"timestring\"]], include_index=False, num_rows_per_page=15)\n",
        "# history_df[[\"text\", \"timestring\"]].tail(2)\n",
        "# history_df[[\"text\", \"timestring\"]]\n",
        "# history_df.head()\n",
        "# history_df.tail()\n",
        "# data_table.DataTable(history_df, num_rows_per_page=15)\n",
        "# data_table.DataTable(history_df[[\"text\", \"timestring\"]], include_index=False, num_rows_per_page=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCm4rp85hGem",
        "cellView": "form"
      },
      "source": [
        "#@markdown tools to change history-view structure\n",
        "# argys = 'timestring,model_name,model_mixing,model2_name,proportion,text,steps,n_batches,fix_coordinates,seed,sample_filetype,cutout_method,u_cutn,u_cut_pow,u_step_size,u_t_ran,u_beta_a,u_beta_b' #an unquoted-variable string in quotes\n",
        "\n",
        "# #tool to create quoted variable list from string of nonquoted variables. \n",
        "# argys_quoted = argys.replace(\",\", \"','\") #add '' around commas\n",
        "# argys_quoted = f\"'{argys_quoted}'\" #add '' on outside\n",
        "# #results can be manually placed into variables above. useful for custom title list and to help maintain spreadsheet formatting (avoiding dict for now)\n",
        "# print(argys_quoted) #goes in data_l (you can change names for custom titles)\n",
        "# print(argys)  #goes in data_ev_l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk10LT6Ig5uv"
      },
      "source": [
        "‚úã Note: this notebook likes to crash often.\n",
        "\n",
        "Create an editable copy for yourself with `File > Save a Copy in Drive`.\n",
        "\n",
        "**To Run this Colab Notebook**, select `Runtime` in the top toolbar menu, then `Run All (Ctrl+F9)`.   \n",
        "  \n",
        "**To Run/Stop individual Code Blocks:** Click the `‚ñ∂` or `‚èπ` button in the upper-left of each code block. It will appear when hovering. These buttons are only functional at the top of the code block, and if you scroll too far down (e.g. inside the results-output), they become non-functional. `spinning` button means the block is running and `checkerd-spinning` means the block is queued.\n",
        "  \n",
        "You can doublecheck settings with `Runtime > Change Runstime Type`:  \n",
        "* `Runtime Shape` should be `High-RAM` if you want to use a `cutn = 128` for the most detailed output. \n",
        "* `‚òë Omit code cell output when saving this notebook` should be checked so that your `.ipynb` files do not grow massive in size.\n",
        "\n",
        "Sometimes, code may fail to run correctly. Colab will also softcrash if your network cannot keep up with the stream of (uncompressed) images. Try `‚ñ∂` running the cell-block again usually fixes this. If Google Colab is still acting weird, try a refresh with `F5` or setting a larger `show_every_n_steps`. If that does not work, try `Runtime > Restart Runtime` or `Runtime > Factory Restart Runtime`. If it is still fails, try a selecting a different `model`, a different `cutout_method` or a shorter `text` prompt.\n",
        "\n",
        "Occationally, this code will give a \"client side CUDA error\", in which case, a factory reset and Run All with identical settings (and a new random seed) seems to fix.\n",
        "  \n"
      ]
    }
  ]
}